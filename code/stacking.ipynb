{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking code for last-man-standing hackathon by AnalyticsVidhya\n",
    "### Code created on : 31st Jan 2015 | Author: Bargava\n",
    "\n",
    "Link to competition\n",
    "http://datahack.analyticsvidhya.com/contest/last-man-standing\n",
    "\n",
    "#### Solution Approach\n",
    "In this, we will will split the training dataset into two. With the first half of the dataset, we will build models that will be used to predict on the second half and will be used as features, along with original features of second half of the training set to build the final classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read train, test and samplesub datasets\n",
    "train = pd.read_csv(\"../data/trainUpdated.csv\")\n",
    "test = pd.read_csv(\"../data/testUpdated.csv\")\n",
    "samplesub = pd.read_csv(\"../data/samplesubUpdated.csv\")\n",
    "label = pd.read_csv(\"../data/labelsUpdated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert the inputs to numpy array\n",
    "train = np.array(train)\n",
    "test = np.array(test)\n",
    "label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the list of indices for first layer and second layer stacking\n",
    "layer_1_indices = StratifiedShuffleSplit(label, test_size=0.5, n_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for train_1_index, train_2_index in layer_1_indices:\n",
    "    layer_1_train, layer_2_train = train[train_1_index], train[train_2_index] \n",
    "    layer_1_label, layer_2_label = label[train_1_index], label[train_2_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's check and find out proportions of the classes in train, layer_1_train and layer_2_train\n",
    "#This is to see if stratified sampling has happened properly\n",
    "print \"train:\", np.unique(label, return_counts=True)[1].astype(float)/label.shape[0]\n",
    "print \"layer_1_train:\",  np.unique(layer_1_label, return_counts=True)[1].astype(float)/layer_1_label.shape[0]\n",
    "print \"layer_2_train:\",  np.unique(layer_2_label, return_counts=True)[1].astype(float)/layer_2_label.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 1 Models\n",
    "\n",
    "# Model 1: *shallow* `xgboost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"min_child_weight\"] = 10\n",
    "params[\"subsample\"] = 0.5\n",
    "params[\"colsample_bytree\"] = 0.5\n",
    "params[\"silent\"] = 0\n",
    "params[\"max_depth\"] = 4\n",
    "params[\"nthread\"] = 6\n",
    "params[\"gamma\"] = 3\n",
    "params[\"objective\"] = \"multi:softmax\"\n",
    "params[\"num_class\"] = 3\n",
    "params[\"verbose\"] = 2\n",
    "params[\"eta\"] = 0.3\n",
    "params[\"base_score\"] = 0\n",
    "params[\"eval_metric\"] = \"merror\"\n",
    "params[\"seed\"] = 13\n",
    "\n",
    "plst = list(params.items())\n",
    "num_rounds = 1200\n",
    "\n",
    "xgtrain_pv = xgb.DMatrix(np.array(layer_1_train).astype(float), label=np.array(layer_1_label).astype(float))\n",
    "watchlist = [(xgtrain_pv, 'train')]\n",
    "\n",
    "model_1 = xgb.train(plst, xgtrain_pv, num_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: *deep* `xgboost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"min_child_weight\"] = 10\n",
    "params[\"subsample\"] = 0.5\n",
    "params[\"colsample_bytree\"] = 0.5\n",
    "params[\"silent\"] = 0\n",
    "params[\"max_depth\"] = 14\n",
    "params[\"nthread\"] = 6\n",
    "params[\"gamma\"] = 3\n",
    "params[\"objective\"] = \"multi:softmax\"\n",
    "params[\"num_class\"] = 3\n",
    "params[\"verbose\"] = 2\n",
    "params[\"eta\"] = 0.3\n",
    "params[\"base_score\"] = 0\n",
    "params[\"eval_metric\"] = \"merror\"\n",
    "params[\"seed\"] = 13\n",
    "\n",
    "plst = list(params.items())\n",
    "num_rounds = 400\n",
    "\n",
    "xgtrain_pv = xgb.DMatrix(np.array(layer_1_train).astype(float), label=np.array(layer_1_label).astype(float))\n",
    "watchlist = [(xgtrain_pv, 'train')]\n",
    "\n",
    "model_2 = xgb.train(plst, xgtrain_pv, num_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: *shallow* `RandomForest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_3 = RandomForestClassifier(n_estimators=1200, max_depth=4, \\\n",
    "                                 max_features=5, min_samples_split=10, min_samples_leaf=5, \\\n",
    "                                 oob_score=True, n_jobs=6)\n",
    "\n",
    "model_3.fit(layer_1_train, np.ravel(layer_1_label))\n",
    "model_3.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4: *deep* `RandomForest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_4 = RandomForestClassifier(n_estimators=400, max_depth=14, \\\n",
    "                                 max_features=5, min_samples_split=10, min_samples_leaf=5, \\\n",
    "                                 oob_score=True, n_jobs=6)\n",
    "\n",
    "model_4.fit(layer_1_train, np.ravel(layer_1_label))\n",
    "model_4.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5: *shallow* `ExtraTrees`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_5 = ExtraTreesClassifier(n_estimators=1200, max_depth=4, \\\n",
    "                                 max_features=5, min_samples_split=10, min_samples_leaf=5, \\\n",
    "                                 oob_score=True, n_jobs=6, bootstrap=True)\n",
    "\n",
    "model_5.fit(layer_1_train, np.ravel(layer_1_label))\n",
    "model_5.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6: *deep* `ExtraTrees`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_6 = ExtraTreesClassifier(n_estimators=400, max_depth=14, \\\n",
    "                                 max_features=5, min_samples_split=10, min_samples_leaf=5, \\\n",
    "                                 oob_score=True, n_jobs=6, bootstrap=True)\n",
    "\n",
    "model_6.fit(layer_1_train, np.ravel(layer_1_label))\n",
    "model_6.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 7: `L2 Logistic Regression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(layer_1_train.astype(float))\n",
    "train_1_layer_lr = scaler.transform(layer_1_train.astype(float))\n",
    "model_7 = LogisticRegression(C=0.01, penalty=\"l2\", n_jobs=6, verbose=1)\n",
    "model_7.fit(train_1_layer_lr, np.ravel(layer_1_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_1_predict = model_1.predict(xgb.DMatrix(np.array(layer_2_train).astype(float)))\n",
    "model_2_predict = model_2.predict(xgb.DMatrix(np.array(layer_2_train).astype(float)))\n",
    "model_3_predict = model_3.predict_proba(layer_2_train)\n",
    "model_4_predict = model_4.predict_proba(layer_2_train)\n",
    "model_5_predict = model_5.predict_proba(layer_2_train)\n",
    "model_6_predict = model_6.predict_proba(layer_2_train)\n",
    "train_2_layer_lr = scaler.transform(layer_2_train.astype(float))\n",
    "model_7_predict = model_7.predict_proba(train_2_layer_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combine all predictions and the original features into a \n",
    "layer_2_train_consolidated = np.concatenate((layer_2_train,\n",
    "                               np.column_stack((model_1_predict,\n",
    "                               model_2_predict)),\n",
    "                               model_3_predict,\n",
    "                               model_4_predict,\n",
    "                               model_5_predict,\n",
    "                               model_6_predict,\n",
    "                               model_7_predict), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_2_train_consolidated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 2 Model\n",
    "\n",
    "### `xgboost` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"min_child_weight\"] = 10\n",
    "params[\"subsample\"] = 0.5\n",
    "params[\"colsample_bytree\"] = 0.5\n",
    "params[\"silent\"] = 0\n",
    "params[\"max_depth\"] = 8\n",
    "params[\"nthread\"] = 6\n",
    "params[\"gamma\"] = 3\n",
    "params[\"objective\"] = \"multi:softmax\"\n",
    "params[\"num_class\"] = 3\n",
    "params[\"verbose\"] = 2\n",
    "params[\"eta\"] = 0.3\n",
    "params[\"base_score\"] = 0\n",
    "params[\"eval_metric\"] = \"merror\"\n",
    "params[\"seed\"] = 13\n",
    "\n",
    "plst = list(params.items())\n",
    "num_rounds = 400\n",
    "\n",
    "xgtrain_pv = xgb.DMatrix(np.array(layer_2_train_consolidated).astype(float), label=np.array(layer_2_label).astype(float))\n",
    "watchlist = [(xgtrain_pv, 'train')]\n",
    "\n",
    "model_layer_2 = xgb.train(plst, xgtrain_pv, num_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_1_predict = model_1.predict(xgb.DMatrix(np.array(test).astype(float)))\n",
    "model_2_predict = model_2.predict(xgb.DMatrix(np.array(test).astype(float)))\n",
    "model_3_predict = model_3.predict_proba(test)\n",
    "model_4_predict = model_4.predict_proba(test)\n",
    "model_5_predict = model_5.predict_proba(test)\n",
    "model_6_predict = model_6.predict_proba(test)\n",
    "test_layer_lr = scaler.transform(test.astype(float))\n",
    "model_7_predict = model_7.predict_proba(test_layer_lr)\n",
    "\n",
    "#combine all predictions and the original features into a \n",
    "layer_2_test_consolidated = np.concatenate((test,\n",
    "                               np.column_stack((model_1_predict,\n",
    "                               model_2_predict)),\n",
    "                               model_3_predict,\n",
    "                               model_4_predict,\n",
    "                               model_5_predict,\n",
    "                               model_6_predict,\n",
    "                               model_7_predict), axis=1)\n",
    "\n",
    "layer_2_predict = model_layer_2.predict(xgb.DMatrix(np.array(layer_2_test_consolidated).astype(float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(layer_2_predict, return_counts=True)[1].astype(float)/layer_2_predict.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samplesub.columns = [\"ID\"]\n",
    "samplesub[\"Crop_Damage\"] = model_1_predict\n",
    "#Write the prediction to a csv\n",
    "samplesub.to_csv(\"../submission/submission_31jan_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#On LB : gives a score of 0.846"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
